#!/usr/bin/env python

'INFER-POP-PARAMS -- infer population parameters based on population model and observations'
__usage__ = 'infer-pop-params pop_params_prior.csv likelihood1.csv likelihood2.csv ... [-C bns bns ... -l num_like -p pop_model_bns pop_model_nsbh -P param1,param2,...+shape,lb,ub,... -F fixed_param,fixed_val -D shape,lb,ub -B beta,lb,ub -f select_func -S m1lb,m1ub,m2lb,m2ub,dllb,dlub -s num_sel -o /path/to/output.csv -t target_num_post_samples -w num_walkers -b num_burnin -v]'
__author__ = 'Philippe Landry (pgjlandry@gmail.com)'
__date__ = '01-2021'

from argparse import ArgumentParser
import numpy as np
import numpy.random
from time import time
import os
import emcee
import sodapop.populations as pop
import sodapop.parameters as prm
import sodapop.priors as pri
import sodapop.select as sel
import sodapop.emcee as mc
import sodapop.diagnostics as dgn

parser = ArgumentParser(description=__doc__)
parser.add_argument('popparams')
parser.add_argument('likedata', nargs='+')
parser.add_argument('-C', '--clas', help='bns or nsbh classification of each event, DEFAULT="bns bns nsbh ..."', default=False, nargs='+')
parser.add_argument('-c', '--col', help='name of mass1, mass2, distance and likelihood data columns to use, DEFAULT="m1 m2 dL z likelihood"', default=['m1','m2','dL','z','likelihood'], nargs='+')
parser.add_argument('-l', '--nlike', help='number of likelihood samples to use, DEFAULT=1e3', default=1e3)
parser.add_argument('-p', '--popmod', help='name of BNS and NSBH population models, DEFAULT="unif_m1m2 unif_m1_unif_m2_qpair"', default=["unif_m1m2","unif_m1_unif_m2_qpair"], nargs=2)
parser.add_argument('-P', '--priors', help='prior distribution and hyperparameters for each population parameter, DEFAULT="mmin,mmax+flat12,1.,1.5,1.5,3."', default="mmin,mmax+flat12,1.,1.5,1.5,3.", nargs='+')
parser.add_argument('-F', '--fixed', help='fixed population parameter values (e.g. mmin,1.0), DEFAULT=False', default=False, nargs='+')
parser.add_argument('-D', '--distprior', help='prior distribution for the distance, DEFAULT="quad,0.,1000."', default="quad,0.,1000.")
parser.add_argument('-B', '--bhpop', help='population parameters for fixed BH mass model (e.g. mmin mmax beta), DEFAULT=3. 60. 2.', default=[3.,60.,2.], nargs='+')
parser.add_argument('-f', '--selfunc', help='name of selection function, DEFAULT=False', default=False)
parser.add_argument('-S', '--selpriors', help='ranges in m1,m2 and dL for integrating selection function, DEFAULT="1.,60.,1.,3.,0.,1000."', default="1.,60.,1.,3.,0.,1000.")
parser.add_argument('-s', '--nsel', help='number of samples to use for integrating selection function, DEFAULT=10000', default=1e4)
parser.add_argument('-t', '--npost', help='target number of posterior samples, DEFAULT=10000', default=1e4)
parser.add_argument('-w', '--nwalk', help='number of MCMC chains, DEFAULT=15', default=15)
parser.add_argument('-b', '--nburn', help='number of burn-in samples, DEFAULT=1000', default=1e3)
parser.add_argument('-o', '--outpath', help='path for output population parameter samples, DEFAULT=AUTO', default=False)
parser.add_argument('-d', '--delim', help='delimiter for data file, DEFAULT=","', default=',')
parser.add_argument('--batch', help='start line of population parameters file for batch mode operation, DEFAULT=False', default=False)
parser.add_argument('--diag', help='produce diagnostic plots, DEFAULT=False', action='store_true', default=False)
parser.add_argument('-v', '--verbose', action='store_true', default=False)
args = parser.parse_args()

if args.outpath: out_path = args.outpath
else: out_path = os.path.dirname(args.likedata[0])+'/'+args.popmod[0]+'.csv'

N_WALKERS = int(args.nwalk)
N_BURNIN = int(args.nburn)
N_POSTS = int(args.npost)
N_SELECT = int(args.nsel)
N_LIKES = int(args.nlike)

if args.verbose: print('walkers: {0}, burn in: {1}, target posterior samples: {2}, likelihood samples: {3}, selection effect samples: {4}'.format(N_WALKERS,N_BURNIN,N_POSTS,N_LIKES,N_SELECT))

### LOAD POPULATION MODEL AND PRIORS

pop_models_list = [pop.get_pop_prior(popmod) for popmod in args.popmod]
pop_models = {'bns': pop_models_list[0], 'nsbh': pop_models_list[1]} # get population models

pop_param_names = (pop.get_pop_params(args.popmod[0])).split(',') # get population parameters from first-specified population model, check that they match what's in the prior samples later

lambdabh = [float(bhparam) for bhparam in args.bhpop] # fixed nsbh population model parameters

select_func, selectsamps = sel.load_select_effect(args.selfunc, args.selpriors, N_SELECT) # selection function and mc samples for evaluating detection fraction

dist_func, dist_params = prm.load_dist_prior(args.distprior) # load distance prior

prior_dict = prm.create_prior_dict(args.priors) # store population prior hyperparameters in a dictionary

if args.fixed and args.fixed != ['False']:
	fixed_params, fixed_dict = prm.create_fixed_dict(args.fixed) # store fixed population parameters in a dictionary
else:
	fixed_params = []
	fixed_dict = {}

lambda_dict = prm.create_lambda_dict(pop_param_names,fixed_params,fixed_dict) # store population parameters being inferred in a dictionary

detfrac_dict = {} # initialize dictionaries to save detection fraction and likelihood
likelihood_dict = {}

### LOAD LIKELIHOOD SAMPLES

if args.verbose: print('Loading likelihood data...')

likesamps = []
for likedata in args.likedata:

	data = np.genfromtxt(likedata,names=True,dtype=None,delimiter=args.delim)
	if args.verbose: print(likedata)
	
	wts = data[args.col[-1]]

	m1s = data[args.col[0]]
	m2s = data[args.col[1]]
	dls = data[args.col[2]]
	zs = data[args.col[3]]
	ids = np.random.choice(range(len(m1s)),N_LIKES,replace=True,p=wts/np.sum(wts))
	samps = [np.array(m1s[ids]),np.array(m2s[ids]),np.array(dls[ids]),np.array(zs[ids])]
	
	likesamps.append(samps)

if args.clas == False or args.clas == 'False': obs_classes = ['bns' for likedata in args.likedata]
else: obs_classes = [str(clas) for clas in args.clas]

### INFER POPULATION

pop_params = np.genfromtxt(args.popparams, names=True, dtype=None, delimiter=args.delim, encoding=None) # load population parameter prior samples
param_names = list(pop_params.dtype.names)
num_prior_samps = len(pop_params[param_names[0]])
assert(set(param_names) == set(pop_param_names)) # make sure population model used for prior samples agrees with one being inferred
for param in param_names:
	if param in fixed_params: param_names.remove(param) # don't infer fixed parameters

if args.batch: ini_idxs = range(int(args.batch),N_WALKERS+int(args.batch)) # initialize emcee walkers
else: ini_idxs = np.random.choice(range(num_prior_samps),N_WALKERS,replace=False)
ini_samps = np.array([pop_params[param_name][ini_idxs] for param_name in param_names]).T
ndims = ini_samps.shape[1] # number of pop params to infer

mc.loglikelihood.ncalls = 0 # initialize likelihood call counter
argslist = (lambda_dict, lambdabh, likesamps, obs_classes, pop_models, selectsamps, select_func, prior_dict, dist_func, dist_params, detfrac_dict, likelihood_dict) # arguments to pass to posterior besides pop parameters lambdaa

dt = [("lambda", list), ("log_like", float), ("log_detfrac", float)]
sampler = emcee.EnsembleSampler(N_WALKERS, ndims, mc.logposterior, args=argslist, blobs_dtype=dt) # initialize emcee sampler

t0 = time() # do the mcmc with emcee
sampler.run_mcmc(ini_samps, N_POSTS+N_BURNIN);
t1 = time()
timeemcee = (t1-t0)
if args.verbose: print("emcee ran for {} seconds".format(timeemcee)) 

acls = sampler.get_autocorr_time(quiet=True) # get chain autocorrelation lengths
if args.verbose: print("autocorrelation lengths for population parameters are {0}".format(acls))

if args.diag:
	dgn.trace_plot(sampler.chain, pop_param_names, fixed_params, os.path.dirname(out_path)+'/traces.png', N_BURNIN)

if np.all(np.isfinite(acls)):
	samples_emcee = sampler.chain[:, N_BURNIN::int(max(acls)), :].reshape((-1, ndims))
	if args.verbose: print("number of independent samples is {0}".format(len(samples_emcee))) # thin out chain to independent samples only
else:
	if args.verbose: print("error! could not thin chain, raw number of samples is {0}".format(len(sampler.chain[:, N_BURNIN:, :].reshape((-1, ndims)))))
	assert(np.all(np.isfinite(acls)))

ess = int(len(samples_emcee) / timeemcee) # repor effective samples per second
if args.verbose: print("effective samples per second is {0}".format(ess))

blobs = sampler.get_blobs(flat=True) # get likelihoods and detection fractions
lambdas = blobs['lambda']
loglikes = blobs['log_like']
logdetfracs = blobs['log_detfrac']

like_dict, detfrac_dict = {}, {}
for sample,loglike,logdetfrac in zip(lambdas,loglikes,logdetfracs):
	like_dict[','.join([str(s) for s in sample])] = loglike
	detfrac_dict[','.join([str(s) for s in sample])] = logdetfrac

if args.diag:
	dgn.corner_plot(samples_emcee, pop_param_names, fixed_params, os.path.dirname(out_path)+'/corner.png')

### SAVE POPULATION POSTERIOR SAMPLES

j = 0
samples_out = []
for param in pop_param_names:
	if param in fixed_params: samples_out += [np.full(len(samples_emcee),fixed_dict[param])]
	else:
		samples_out += [samples_emcee[:,j]]
		j += 1

loglikes_out = []
logdetfracs_out = []		
for sample in samples_emcee:
	loglikes_out += [like_dict[','.join([str(s) for s in sample])]]
	logdetfracs_out += [detfrac_dict[','.join([str(s) for s in sample])]]

posts = np.ones((len(samples_emcee),1)) # explicitly print equal weights
samples_out += [loglikes_out,logdetfracs_out,posts]

post_samps = np.column_stack(samples_out)
col_names = ','.join(pop_param_names)+',log_like,log_detfrac,weight'
np.savetxt(out_path,post_samps[np.isfinite(post_samps).all(axis=1)],header=col_names,comments='',delimiter=',')

